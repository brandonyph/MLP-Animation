theme_bw()+
theme(legend.position = "none")
##function to add all arrows
for (i in 1:(layers - 1)) {
for (j in 1:(array[i])) {
for (o in 1:(array[i + 1])) {
#print(paste0("i-", i))
#print(paste0("j-", j))
#print(paste0("o-", o))
x1 <- i
y1 <- df4[df4$layers == i, ]$nodes[j]
x2 <- i + 1
y2 <- df4[df4$layers == (i + 1), ]$nodes[o]
coor <- data.frame(x = c(x1, x2), y = c(y1, y2))
weights <- Overall[[k]]$weight[[x1+1]][[j,o]]
p <- p + geom_path(data = coor, aes(x = x, y = y), alpha = weights, size=0.5,color="blue")
}
}
}
print(p)
ggsave(paste0("plot-",k,".jpg"),
width = 1920,
height = 1080,
units = c("px"),
dpi = 300)
}
###Create neural network
library(tensorflow)
library(keras)
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}
copydata <- function(array,n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
train_data <- unlist(iris[,1:4])
dim(train_data) <- c(150,4)
train_data  <- train_data  %>% scale() %>% normalize()
train_data <- train_data %>% copydata(5)
train_label <- as.factor(iris[,5])
train_label <- train_label %>% as.numeric() %>% as.matrix()
train_label <- train_label - 1
# Convert labels to categorical one-hot encoding
train_label <- to_categorical(train_label,num_classes = 3)
train_label <- train_label %>% copydata(5)
rm(model)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(4)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = array[3], activation = "sigmoid") %>%
layer_dense(units = array[4], activation = "sigmoid") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
weight <- list()
bias <- list()
model_stat <- list()
Overall <- list()
acc <- list()
for(e in 1:20){
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 1,
use_multiprocessing=TRUE,
batch_size = 30
)
for (l in 1:length(array)){
weight[[l]] <- as.matrix(model$layers[[l]]$weights[[1]])
bias[[l]]   <- as.matrix(model$layers[[l]]$weights[[2]])
}
model_stat$weight <- weight
model_stat$bias <- bias
Overall[[e]] <- model_stat
acc[[e]] <- model$history$history$accuracy
}
df
df3 <- df
for(p in 2:length(Overall)) {
df2 <- df
df2$epoch <- p
for (l in 1:layers) {
df2$sizes[df2$layers==l] <-  Overall[[p]]$bias[[l]]
}
df3 <- rbind(df3, df2)
}
# Increase size for better plotting
df3$sizes <- normalize(df3$sizes)
library(ggplot2)
library(gganimate)
for (k in 1:20) {
df4 <- df3[df3$epoch==k,]
## Create base Canvas
p <-
ggplot(data = df4)  + geom_point(aes(x = layers, y = nodes, size = sizes*2,color="red")) +
labs(x = "Layers",
y = "Nodes") +
ggtitle(paste0("Epoch:", k,"  ","Accurancy:",round(acc[[k]],2))) +
theme_bw()+
theme(legend.position = "none")
##function to add all arrows
for (i in 1:(layers - 1)) {
for (j in 1:(array[i])) {
for (o in 1:(array[i + 1])) {
#print(paste0("i-", i))
#print(paste0("j-", j))
#print(paste0("o-", o))
x1 <- i
y1 <- df4[df4$layers == i, ]$nodes[j]
x2 <- i + 1
y2 <- df4[df4$layers == (i + 1), ]$nodes[o]
coor <- data.frame(x = c(x1, x2), y = c(y1, y2))
weights <- Overall[[k]]$weight[[x1+1]][[j,o]]
p <- p + geom_path(data = coor, aes(x = x, y = y), alpha = weights, size=0.5,color="blue")
}
}
}
print(p)
ggsave(paste0("plot-",k,".jpg"),
width = 1920,
height = 1080,
units = c("px"),
dpi = 300)
}
###Create neural network
library(tensorflow)
library(keras)
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}
copydata <- function(array,n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
train_data <- unlist(iris[,1:4])
dim(train_data) <- c(150,4)
train_data  <- train_data  %>% scale() %>% normalize()
train_data <- train_data %>% copydata(3)
train_label <- as.factor(iris[,5])
train_label <- train_label %>% as.numeric() %>% as.matrix()
train_label <- train_label - 1
# Convert labels to categorical one-hot encoding
train_label <- to_categorical(train_label,num_classes = 3)
train_label <- train_label %>% copydata(5)
rm(model)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(4)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = array[3], activation = "sigmoid") %>%
layer_dense(units = array[4], activation = "sigmoid") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
weight <- list()
bias <- list()
model_stat <- list()
Overall <- list()
acc <- list()
for(e in 1:30){
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 1,
use_multiprocessing=TRUE,
batch_size = 30
)
for (l in 1:length(array)){
weight[[l]] <- as.matrix(model$layers[[l]]$weights[[1]])
bias[[l]]   <- as.matrix(model$layers[[l]]$weights[[2]])
}
model_stat$weight <- weight
model_stat$bias <- bias
Overall[[e]] <- model_stat
acc[[e]] <- model$history$history$accuracy
}
###Create neural network
library(tensorflow)
library(keras)
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}
copydata <- function(array,n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
train_data <- unlist(iris[,1:4])
dim(train_data) <- c(150,4)
train_data  <- train_data  %>% scale() %>% normalize()
train_data <- train_data %>% copydata(3)
train_label <- as.factor(iris[,5])
train_label <- train_label %>% as.numeric() %>% as.matrix()
train_label <- train_label - 1
# Convert labels to categorical one-hot encoding
train_label <- to_categorical(train_label,num_classes = 3)
train_label <- train_label %>% copydata(3)
rm(model)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(4)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = array[3], activation = "sigmoid") %>%
layer_dense(units = array[4], activation = "sigmoid") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
weight <- list()
bias <- list()
model_stat <- list()
Overall <- list()
acc <- list()
for(e in 1:30){
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 1,
use_multiprocessing=TRUE,
batch_size = 30
)
for (l in 1:length(array)){
weight[[l]] <- as.matrix(model$layers[[l]]$weights[[1]])
bias[[l]]   <- as.matrix(model$layers[[l]]$weights[[2]])
}
model_stat$weight <- weight
model_stat$bias <- bias
Overall[[e]] <- model_stat
acc[[e]] <- model$history$history$accuracy
}
df
df3 <- df
for(p in 2:length(Overall)) {
df2 <- df
df2$epoch <- p
for (l in 1:layers) {
df2$sizes[df2$layers==l] <-  Overall[[p]]$bias[[l]]
}
df3 <- rbind(df3, df2)
}
# Increase size for better plotting
df3$sizes <- normalize(df3$sizes)
library(ggplot2)
library(gganimate)
for (k in 1:30) {
df4 <- df3[df3$epoch==k,]
## Create base Canvas
p <-
ggplot(data = df4)  + geom_point(aes(x = layers, y = nodes, size = sizes*2,color="red")) +
labs(x = "Layers",
y = "Nodes") +
ggtitle(paste0("Epoch:", k,"  ","Accurancy:",round(acc[[k]],2))) +
theme_bw()+
theme(legend.position = "none")
##function to add all arrows
for (i in 1:(layers - 1)) {
for (j in 1:(array[i])) {
for (o in 1:(array[i + 1])) {
#print(paste0("i-", i))
#print(paste0("j-", j))
#print(paste0("o-", o))
x1 <- i
y1 <- df4[df4$layers == i, ]$nodes[j]
x2 <- i + 1
y2 <- df4[df4$layers == (i + 1), ]$nodes[o]
coor <- data.frame(x = c(x1, x2), y = c(y1, y2))
weights <- Overall[[k]]$weight[[x1+1]][[j,o]]
p <- p + geom_path(data = coor, aes(x = x, y = y), alpha = weights, size=0.5,color="blue")
}
}
}
print(p)
ggsave(paste0("plot-",k,".jpg"),
width = 1920,
height = 1080,
units = c("px"),
dpi = 300)
}
###Create neural network
library(tensorflow)
library(keras)
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}
copydata <- function(array,n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
train_data <- unlist(iris[,1:4])
dim(train_data) <- c(150,4)
train_data  <- train_data  %>% scale() %>% normalize()
train_data <- train_data %>% copydata(4)
train_label <- as.factor(iris[,5])
train_label <- train_label %>% as.numeric() %>% as.matrix()
train_label <- train_label - 1
# Convert labels to categorical one-hot encoding
train_label <- to_categorical(train_label,num_classes = 3)
train_label <- train_label %>% copydata(4)
rm(model)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(4)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = array[3], activation = "sigmoid") %>%
layer_dense(units = array[4], activation = "sigmoid") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
weight <- list()
bias <- list()
model_stat <- list()
Overall <- list()
acc <- list()
for(e in 1:30){
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 1,
use_multiprocessing=TRUE,
batch_size = 30
)
for (l in 1:length(array)){
weight[[l]] <- as.matrix(model$layers[[l]]$weights[[1]])
bias[[l]]   <- as.matrix(model$layers[[l]]$weights[[2]])
}
model_stat$weight <- weight
model_stat$bias <- bias
Overall[[e]] <- model_stat
acc[[e]] <- model$history$history$accuracy
}
###Create neural network
library(tensorflow)
library(keras)
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}
copydata <- function(array,n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
train_data <- unlist(iris[,1:4])
dim(train_data) <- c(150,4)
train_data  <- train_data  %>% scale() %>% normalize()
train_data <- train_data %>% copydata(4)
train_label <- as.factor(iris[,5])
train_label <- train_label %>% as.numeric() %>% as.matrix()
train_label <- train_label - 1
# Convert labels to categorical one-hot encoding
train_label <- to_categorical(train_label,num_classes = 3)
train_label <- train_label %>% copydata(4)
rm(model)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(4)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = array[3], activation = "sigmoid") %>%
layer_dense(units = array[4], activation = "sigmoid") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
weight <- list()
bias <- list()
model_stat <- list()
Overall <- list()
acc <- list()
for(e in 1:30){
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 1,
use_multiprocessing=TRUE,
batch_size = 50
)
for (l in 1:length(array)){
weight[[l]] <- as.matrix(model$layers[[l]]$weights[[1]])
bias[[l]]   <- as.matrix(model$layers[[l]]$weights[[2]])
}
model_stat$weight <- weight
model_stat$bias <- bias
Overall[[e]] <- model_stat
acc[[e]] <- model$history$history$accuracy
}
###Create neural network
library(tensorflow)
library(keras)
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}
copydata <- function(array,n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
train_data <- unlist(iris[,1:4])
dim(train_data) <- c(150,4)
train_data  <- train_data  %>% scale() %>% normalize()
train_data <- train_data %>% copydata(4)
train_label <- as.factor(iris[,5])
train_label <- train_label %>% as.numeric() %>% as.matrix()
train_label <- train_label - 1
# Convert labels to categorical one-hot encoding
train_label <- to_categorical(train_label,num_classes = 3)
train_label <- train_label %>% copydata(4)
rm(model)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(4)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = array[3], activation = "sigmoid") %>%
layer_dense(units = array[4], activation = "sigmoid") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
weight <- list()
bias <- list()
model_stat <- list()
Overall <- list()
acc <- list()
for(e in 1:30){
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 1,
use_multiprocessing=TRUE,
batch_size = 20
)
for (l in 1:length(array)){
weight[[l]] <- as.matrix(model$layers[[l]]$weights[[1]])
bias[[l]]   <- as.matrix(model$layers[[l]]$weights[[2]])
}
model_stat$weight <- weight
model_stat$bias <- bias
Overall[[e]] <- model_stat
acc[[e]] <- model$history$history$accuracy
}
df
df3 <- df
for(p in 2:length(Overall)) {
df2 <- df
df2$epoch <- p
for (l in 1:layers) {
df2$sizes[df2$layers==l] <-  Overall[[p]]$bias[[l]]
}
df3 <- rbind(df3, df2)
}
# Increase size for better plotting
df3$sizes <- normalize(df3$sizes)
library(ggplot2)
library(gganimate)
for (k in 1:30) {
df4 <- df3[df3$epoch==k,]
## Create base Canvas
p <-
ggplot(data = df4)  + geom_point(aes(x = layers, y = nodes, size = sizes*2,color="red")) +
labs(x = "Layers",
y = "Nodes") +
ggtitle(paste0("Epoch:", k,"  ","Accurancy:",round(acc[[k]],2))) +
theme_bw()+
theme(legend.position = "none")
##function to add all arrows
for (i in 1:(layers - 1)) {
for (j in 1:(array[i])) {
for (o in 1:(array[i + 1])) {
#print(paste0("i-", i))
#print(paste0("j-", j))
#print(paste0("o-", o))
x1 <- i
y1 <- df4[df4$layers == i, ]$nodes[j]
x2 <- i + 1
y2 <- df4[df4$layers == (i + 1), ]$nodes[o]
coor <- data.frame(x = c(x1, x2), y = c(y1, y2))
weights <- Overall[[k]]$weight[[x1+1]][[j,o]]
p <- p + geom_path(data = coor, aes(x = x, y = y), alpha = weights, size=0.5,color="blue")
}
}
}
print(p)
ggsave(paste0("plot-",k,".jpg"),
width = 1920,
height = 1080,
units = c("px"),
dpi = 300)
}
